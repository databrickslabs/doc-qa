{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>source</th>\n",
       "      <th>cleaned_context</th>\n",
       "      <th>cleaned_context_num_gpt4_tokens</th>\n",
       "      <th>context_num_gpt4_tokens</th>\n",
       "      <th>num_tokens_range</th>\n",
       "      <th>generated_question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\n pyspark.streaming.DStream.groupByKeyAndWind...</td>\n",
       "      <td>https://spark.apache.org/docs/latest/api/pytho...</td>\n",
       "      <td>pyspark.streaming.DStream.groupByKeyAndWindow ...</td>\n",
       "      <td>191</td>\n",
       "      <td>339</td>\n",
       "      <td>200~500</td>\n",
       "      <td>How to use groupByKeyAndWindow in Python to a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\n pyspark.pandas.Series.any\\n [¶](#pyspark-pa...</td>\n",
       "      <td>https://spark.apache.org/docs/latest/api/pytho...</td>\n",
       "      <td>pyspark.pandas.Series.any [](#pysparkpandasser...</td>\n",
       "      <td>204</td>\n",
       "      <td>391</td>\n",
       "      <td>200~500</td>\n",
       "      <td>How do you check if any value is True in a py...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\n pyspark.sql.DataFrameNaFunctions.replace\\n ...</td>\n",
       "      <td>https://spark.apache.org/docs/latest/api/pytho...</td>\n",
       "      <td>pyspark.sql.DataFrameNaFunctions.replace [](#p...</td>\n",
       "      <td>477</td>\n",
       "      <td>1007</td>\n",
       "      <td>500~1200</td>\n",
       "      <td>How to replace values in a DataFrame column u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\n pyspark.pandas.Series.dt.is\\_year\\_end\\n [¶...</td>\n",
       "      <td>https://spark.apache.org/docs/latest/api/pytho...</td>\n",
       "      <td>pyspark.pandas.Series.dt.is_year_end [](#pyspa...</td>\n",
       "      <td>230</td>\n",
       "      <td>337</td>\n",
       "      <td>200~500</td>\n",
       "      <td>How do you check if a datetime Series in Pand...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\n`dateadd`\\n (days) function\\n===============...</td>\n",
       "      <td>https://docs.databricks.com/sql/language-manua...</td>\n",
       "      <td>dateadd (days) function Returns the date numDa...</td>\n",
       "      <td>173</td>\n",
       "      <td>307</td>\n",
       "      <td>200~500</td>\n",
       "      <td>How do you use the dateadd function in Databr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>\\n`regexp\\_count`\\n function\\n================...</td>\n",
       "      <td>https://docs.databricks.com/sql/language-manua...</td>\n",
       "      <td>regexp_count function Applies to: [check marke...</td>\n",
       "      <td>227</td>\n",
       "      <td>411</td>\n",
       "      <td>200~500</td>\n",
       "      <td>How can I use the regexp_count function in Py...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>\\n`array\\_union`\\n function\\n=================...</td>\n",
       "      <td>https://docs.databricks.com/sql/language-manua...</td>\n",
       "      <td>array_union function Applies to: [check marked...</td>\n",
       "      <td>149</td>\n",
       "      <td>264</td>\n",
       "      <td>200~500</td>\n",
       "      <td>How to use the array_union function in Databr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>\\n Genomics guide\\n================\\n Importan...</td>\n",
       "      <td>https://docs.databricks.com/archive/genomics/i...</td>\n",
       "      <td>Genomics guide Important This documentation ha...</td>\n",
       "      <td>363</td>\n",
       "      <td>624</td>\n",
       "      <td>500~1200</td>\n",
       "      <td>How do I perform secondary analysis pipelines...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>\\n p\\n \\n[org](../../../../index.html) \\n .\\n ...</td>\n",
       "      <td>https://spark.apache.org/docs/latest/api/scala...</td>\n",
       "      <td>p [org](../../../../index.html) . [apache](../...</td>\n",
       "      <td>421</td>\n",
       "      <td>1051</td>\n",
       "      <td>500~1200</td>\n",
       "      <td>How to generate random data in Spark using th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>\\n Rating\\n [¶](#rating \"Permalink to this hea...</td>\n",
       "      <td>https://spark.apache.org/docs/latest/api/pytho...</td>\n",
       "      <td>Rating [](#rating Permalink to this headline) ...</td>\n",
       "      <td>379</td>\n",
       "      <td>646</td>\n",
       "      <td>500~1200</td>\n",
       "      <td>How to use the Rating class in PySpark for re...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               context  \\\n",
       "0    \\n pyspark.streaming.DStream.groupByKeyAndWind...   \n",
       "1    \\n pyspark.pandas.Series.any\\n [¶](#pyspark-pa...   \n",
       "2    \\n pyspark.sql.DataFrameNaFunctions.replace\\n ...   \n",
       "3    \\n pyspark.pandas.Series.dt.is\\_year\\_end\\n [¶...   \n",
       "4    \\n`dateadd`\\n (days) function\\n===============...   \n",
       "..                                                 ...   \n",
       "195  \\n`regexp\\_count`\\n function\\n================...   \n",
       "196  \\n`array\\_union`\\n function\\n=================...   \n",
       "197  \\n Genomics guide\\n================\\n Importan...   \n",
       "198  \\n p\\n \\n[org](../../../../index.html) \\n .\\n ...   \n",
       "199  \\n Rating\\n [¶](#rating \"Permalink to this hea...   \n",
       "\n",
       "                                                source  \\\n",
       "0    https://spark.apache.org/docs/latest/api/pytho...   \n",
       "1    https://spark.apache.org/docs/latest/api/pytho...   \n",
       "2    https://spark.apache.org/docs/latest/api/pytho...   \n",
       "3    https://spark.apache.org/docs/latest/api/pytho...   \n",
       "4    https://docs.databricks.com/sql/language-manua...   \n",
       "..                                                 ...   \n",
       "195  https://docs.databricks.com/sql/language-manua...   \n",
       "196  https://docs.databricks.com/sql/language-manua...   \n",
       "197  https://docs.databricks.com/archive/genomics/i...   \n",
       "198  https://spark.apache.org/docs/latest/api/scala...   \n",
       "199  https://spark.apache.org/docs/latest/api/pytho...   \n",
       "\n",
       "                                       cleaned_context  \\\n",
       "0    pyspark.streaming.DStream.groupByKeyAndWindow ...   \n",
       "1    pyspark.pandas.Series.any [](#pysparkpandasser...   \n",
       "2    pyspark.sql.DataFrameNaFunctions.replace [](#p...   \n",
       "3    pyspark.pandas.Series.dt.is_year_end [](#pyspa...   \n",
       "4    dateadd (days) function Returns the date numDa...   \n",
       "..                                                 ...   \n",
       "195  regexp_count function Applies to: [check marke...   \n",
       "196  array_union function Applies to: [check marked...   \n",
       "197  Genomics guide Important This documentation ha...   \n",
       "198  p [org](../../../../index.html) . [apache](../...   \n",
       "199  Rating [](#rating Permalink to this headline) ...   \n",
       "\n",
       "     cleaned_context_num_gpt4_tokens  context_num_gpt4_tokens  \\\n",
       "0                                191                      339   \n",
       "1                                204                      391   \n",
       "2                                477                     1007   \n",
       "3                                230                      337   \n",
       "4                                173                      307   \n",
       "..                               ...                      ...   \n",
       "195                              227                      411   \n",
       "196                              149                      264   \n",
       "197                              363                      624   \n",
       "198                              421                     1051   \n",
       "199                              379                      646   \n",
       "\n",
       "    num_tokens_range                                 generated_question  \n",
       "0            200~500   How to use groupByKeyAndWindow in Python to a...  \n",
       "1            200~500   How do you check if any value is True in a py...  \n",
       "2           500~1200   How to replace values in a DataFrame column u...  \n",
       "3            200~500   How do you check if a datetime Series in Pand...  \n",
       "4            200~500   How do you use the dateadd function in Databr...  \n",
       "..               ...                                                ...  \n",
       "195          200~500   How can I use the regexp_count function in Py...  \n",
       "196          200~500   How to use the array_union function in Databr...  \n",
       "197         500~1200   How do I perform secondary analysis pipelines...  \n",
       "198         500~1200   How to generate random data in Spark using th...  \n",
       "199         500~1200   How to use the Rating class in PySpark for re...  \n",
       "\n",
       "[200 rows x 7 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the environment variables\n",
    "import pandas as pd\n",
    "\n",
    "import dotenv\n",
    "dotenv.load_dotenv('../.env')\n",
    "\n",
    "\n",
    "small_dataset = pd.read_csv(\"../datasets/docs_qa_small_context_200.csv\")\n",
    "\n",
    "small_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:databricks.labs.doc_qa.model_generators.model_generator:Generated total number of batches for prompts: 20\n",
      "INFO:databricks.labs.doc_qa.model_generators.model_generator:Generated total number of results: 20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_rows: 20\n",
      "num_successful_rows: 20\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_successful</th>\n",
       "      <th>error_msg</th>\n",
       "      <th>content</th>\n",
       "      <th>temperature</th>\n",
       "      <th>max_tokens</th>\n",
       "      <th>model_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>To convert strings in a pyspark.pandas Series ...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>100</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>To use the getbit function in Spark SQL to get...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>100</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>To use the dateadd function in Databricks SQL ...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>100</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>To use the max_by aggregate function in Databr...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>100</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>To check if any value is True in a pyspark.pan...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>100</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>To check if a datetime Series in Pandas on Spa...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>100</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>To use the explode function in Python to explo...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>100</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>To use `groupByKeyAndWindow` in Python to appl...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>100</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>To register and use a user-defined function (U...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>100</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>To replace values in a DataFrame column using ...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>100</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>To use the any_value aggregate function in Dat...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>100</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>To use VarcharType in Python, you can follow t...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>100</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>To sort a DataFrame in PySpark by index, you c...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>100</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>To resolve the RESOURCE_LIMIT_EXCEEDED error w...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>100</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>To use the regr_avgx aggregate function in Dat...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>100</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>To load an ORC file into a DataFrame in Python...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>100</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>To drop a database in Spark SQL, you can use t...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>100</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>To load data from CSV files into a Delta Lake ...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>100</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>To convert between PySpark and pandas DataFram...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>100</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>To handle the UNSUPPORTED_SUBQUERY_EXPRESSION_...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>100</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    is_successful error_msg  \\\n",
       "0            True      None   \n",
       "1            True      None   \n",
       "2            True      None   \n",
       "3            True      None   \n",
       "4            True      None   \n",
       "5            True      None   \n",
       "6            True      None   \n",
       "7            True      None   \n",
       "8            True      None   \n",
       "9            True      None   \n",
       "10           True      None   \n",
       "11           True      None   \n",
       "12           True      None   \n",
       "13           True      None   \n",
       "14           True      None   \n",
       "15           True      None   \n",
       "16           True      None   \n",
       "17           True      None   \n",
       "18           True      None   \n",
       "19           True      None   \n",
       "\n",
       "                                              content  temperature  \\\n",
       "0   To convert strings in a pyspark.pandas Series ...          0.5   \n",
       "1   To use the getbit function in Spark SQL to get...          0.5   \n",
       "2   To use the dateadd function in Databricks SQL ...          0.5   \n",
       "3   To use the max_by aggregate function in Databr...          0.5   \n",
       "4   To check if any value is True in a pyspark.pan...          0.5   \n",
       "5   To check if a datetime Series in Pandas on Spa...          0.5   \n",
       "6   To use the explode function in Python to explo...          0.5   \n",
       "7   To use `groupByKeyAndWindow` in Python to appl...          0.5   \n",
       "8   To register and use a user-defined function (U...          0.5   \n",
       "9   To replace values in a DataFrame column using ...          0.5   \n",
       "10  To use the any_value aggregate function in Dat...          0.5   \n",
       "11  To use VarcharType in Python, you can follow t...          0.5   \n",
       "12  To sort a DataFrame in PySpark by index, you c...          0.5   \n",
       "13  To resolve the RESOURCE_LIMIT_EXCEEDED error w...          0.5   \n",
       "14  To use the regr_avgx aggregate function in Dat...          0.5   \n",
       "15  To load an ORC file into a DataFrame in Python...          0.5   \n",
       "16  To drop a database in Spark SQL, you can use t...          0.5   \n",
       "17  To load data from CSV files into a Delta Lake ...          0.5   \n",
       "18  To convert between PySpark and pandas DataFram...          0.5   \n",
       "19  To handle the UNSUPPORTED_SUBQUERY_EXPRESSION_...          0.5   \n",
       "\n",
       "    max_tokens     model_name  \n",
       "0          100  gpt-3.5-turbo  \n",
       "1          100  gpt-3.5-turbo  \n",
       "2          100  gpt-3.5-turbo  \n",
       "3          100  gpt-3.5-turbo  \n",
       "4          100  gpt-3.5-turbo  \n",
       "5          100  gpt-3.5-turbo  \n",
       "6          100  gpt-3.5-turbo  \n",
       "7          100  gpt-3.5-turbo  \n",
       "8          100  gpt-3.5-turbo  \n",
       "9          100  gpt-3.5-turbo  \n",
       "10         100  gpt-3.5-turbo  \n",
       "11         100  gpt-3.5-turbo  \n",
       "12         100  gpt-3.5-turbo  \n",
       "13         100  gpt-3.5-turbo  \n",
       "14         100  gpt-3.5-turbo  \n",
       "15         100  gpt-3.5-turbo  \n",
       "16         100  gpt-3.5-turbo  \n",
       "17         100  gpt-3.5-turbo  \n",
       "18         100  gpt-3.5-turbo  \n",
       "19         100  gpt-3.5-turbo  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from databricks.labs.doc_qa.llm_utils import PromptTemplate\n",
    "from databricks.labs.doc_qa.model_generators.model_generator import OpenAiModelGenerator\n",
    "\n",
    "small_dataset = small_dataset.head(20)\n",
    "\n",
    "prompt_template = PromptTemplate(\"\"\"You are a helpful assistant good at answering questions based on context. Please make sure the answer is correct, comprehensive and reader-friendly. \n",
    "\n",
    "Here is the question: \n",
    "{generated_question}\n",
    "\n",
    "\n",
    "Here is the context:\n",
    "{cleaned_context}\n",
    "\n",
    "Please give your answer:\n",
    "\"\"\")\n",
    "                                 \n",
    "\n",
    "model_generator = OpenAiModelGenerator(prompt_formatter=prompt_template,model_name=\"gpt-3.5-turbo\", concurrency=10)\n",
    "\n",
    "result = model_generator.run_tasks(input_df=small_dataset,temperature=0.5, max_tokens=100)\n",
    "\n",
    "result_df = result.to_dataframe()\n",
    "print(result.summary())\n",
    "result_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "docs3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
